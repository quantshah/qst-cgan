{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reconstructing cat states with a single shot\n",
    "\n",
    "We reconstruct the density matrix of the family of bosonic `cat` codes with a\n",
    "single evaluation of a pre-trained CGAN. The definition of the `cat` states is\n",
    "taken from [Albert et al., 2018](https://www.doi.org/10.1103/PhysRevA.97.032346).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "from qutip import expect, displace, fock_dm, coherent, coherent_dm, fidelity, rand_dm\n",
    "from qutip.wigner import wigner\n",
    "from qutip.visualization import plot_wigner_fock_distribution, plot_wigner\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from qst_cgan.ops import convert_to_real_ops, batched_expect, tf_fidelity, dm_to_tf, convert_to_complex_ops, tf_to_dm\n",
    "from qst_cgan.gan import generator_loss, discriminator_loss, Generator, Discriminator, DensityMatrix, Expectation\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hilbert_size = 32\n",
    "\n",
    "def cat(N, alpha, S=None, mu=None):\n",
    "    \"\"\"\n",
    "    Generates a cat state. For a detailed discussion on the definition\n",
    "    see `Albert, Victor V. et al. “Performance and Structure of Single-Mode Bosonic Codes.” Physical Review A 97.3 (2018) <https://arxiv.org/abs/1708.05010>`_\n",
    "    and `Ahmed, Shahnawaz et al., “Classification and reconstruction of quantum states with neural networks.” Journal <https://arxiv.org/abs/1708.05010>`_\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        N (int): Hilbert size dimension.\n",
    "        alpha (complex64): Complex number determining the amplitude.\n",
    "        S (int): An integer >= 0 determining the number of coherent states used\n",
    "                 to generate the cat superposition. S = {0, 1, 2, ...}.\n",
    "                 corresponds to {2, 4, 6, ...} coherent state superpositions.\n",
    "        mu (int): An integer 0/1 which generates the logical 0/1 encoding of \n",
    "                  a computational state for the cat state.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        cat (:class:`qutip.Qobj`): Cat state density matrix\n",
    "    \"\"\"\n",
    "    if S == None:\n",
    "        S = 0\n",
    "\n",
    "    if mu is None:\n",
    "        mu = 0\n",
    "\n",
    "    kend = 2 * S + 1\n",
    "    cstates = 0 * (coherent(N, 0))\n",
    "\n",
    "    for k in range(0, int((kend + 1) / 2)):\n",
    "        sign = 1\n",
    "\n",
    "        if k >= S:\n",
    "            sign = (-1) ** int(mu > 0.5)\n",
    "\n",
    "        prefactor = np.exp(1j * (np.pi / (S + 1)) * k)\n",
    "\n",
    "        cstates += sign * coherent(N, prefactor * alpha * (-((1j) ** mu)))\n",
    "        cstates += sign * coherent(N, -prefactor * alpha * (-((1j) ** mu)))\n",
    "\n",
    "    rho = cstates * cstates.dag()\n",
    "    return rho.unit()\n",
    "\n",
    "\n",
    "rho = cat(hilbert_size, 2, 1, mu=0)\n",
    "plot_wigner_fock_distribution(rho)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construct the measurement operators\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def husimi_op(hilbert_size: int, beta: complex):\n",
    "    \"\"\"Computes the operator for a Wigner function measurement\n",
    "\n",
    "    Args:\n",
    "        hilbert_size (int): Hilbert size dimension\n",
    "        beta (complex): Phase space point\n",
    "    \"\"\"\n",
    "    return coherent_dm(hilbert_size, beta)\n",
    "\n",
    "xvec = np.linspace(-7.5, 7.5, 32)\n",
    "yvec = np.linspace(-7.5, 7.5, 32)\n",
    "\n",
    "X, Y = np.meshgrid(xvec, yvec)\n",
    "betas = (X + 1j*Y).ravel()\n",
    "m_ops = [husimi_op(hilbert_size, beta) for beta in betas]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup the inputs to the neural network - measurement ops and data\n",
    "\n",
    "We need to convert the QuTiP qunatum objects to TensorFlow objects in order to feed them to the neural network.\n",
    "We take 4281 operators (32 x 32 grid of complex $\\beta$ values in the phase space). Each operator is a complex matrix\n",
    "of size 32 x 32 since we choose a Hilbert space cutoff of 32. \n",
    "\n",
    "Once we convert the measurement operators to a TensorFlow complex tensor, we further separate the real and imaginary parts.\n",
    "The measurement operators are therefore input as real matrix with the last dimension as (`2 x ` number of operators). We have 1024 operators and therefore the input to the neural networks is a $1 \\times  32 \\times 32  \\times 2048$ dimensional tensor containing the real and imaginary parts of the all the complex-valued measurement operators."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 32 # generate 32 states in each round\n",
    "\n",
    "ops = dm_to_tf(m_ops)\n",
    "ops_batch = tf.convert_to_tensor([ops for i in range(batch_size)])\n",
    "A = convert_to_real_ops(ops_batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Conditional Generative Adversarial Network (CGAN)\n",
    "\n",
    "We construct a `Generator` and a `Discriminator` using TensorFlow and train them for state reconstruction in an adversarial fashion. The inputs to the CGAN are the data (`x`) and the measurement operators converted as real matrices `A`. We also construct a `model_dm` that shares weights with the generator to predict the density matrix of the state."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_measurements = 32*32\n",
    "generator = Generator(hilbert_size, num_measurements, noise=0.)\n",
    "discriminator = Discriminator(hilbert_size, num_measurements)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "density_layer_idx = None\n",
    "\n",
    "for i, layer in enumerate(generator.layers): \n",
    "    if \"density_matrix\" in layer._name:\n",
    "        density_layer_idx = i\n",
    "        break\n",
    "\n",
    "model_dm = tf.keras.Model(inputs=generator.input,\n",
    "                          outputs=generator.layers[density_layer_idx].output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let us make a prediction on a set of randomly generated states"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def random_alpha(radius: float, inner_radius=0):\n",
    "    \"\"\"\n",
    "    Generates a random complex value within a circle.\n",
    "\n",
    "    Args:\n",
    "        radius (float): Radius of the phase space\n",
    "        inner_radius (int, optional): [description]. Inner radius.\n",
    "                                                     Defaults to 0\n",
    "\n",
    "    Returns:\n",
    "        complex: A complex value.\n",
    "    \"\"\"\n",
    "    radius = np.random.uniform(inner_radius, radius)\n",
    "    phi = np.random.uniform(-np.pi, np.pi)\n",
    "    return radius * np.exp(1j * phi)\n",
    "\n",
    "\n",
    "def generate_batch(batch_size: int):\n",
    "    \"\"\"Generates a batch of random cat state density matrices\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Batch size\n",
    "\n",
    "    Yields:\n",
    "        [list of Qobj]: A list of density matrices\n",
    "    \"\"\"\n",
    "    rhos = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        alpha = random_alpha(1, 3)\n",
    "        \n",
    "        S = np.random.randint(0, 4)\n",
    "        dm = cat(hilbert_size, alpha=alpha, S=S)\n",
    "        sigma = np.random.uniform(0, 0.9)\n",
    "        sparsity = np.random.uniform(0, 0.9)\n",
    "\n",
    "        rand = rand_dm(hilbert_size, sparsity)\n",
    "        rho  = (1 - sigma)*dm + sigma*(rand)\n",
    "        rho = rho/rho.tr()\n",
    "\n",
    "        rhos.append(rho)\n",
    "\n",
    "    yield rhos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def show_comparision(predicted_rho:list, rho_true:list):\n",
    "    \"\"\"Plot a comparision between predictions and true density matrices\n",
    "\n",
    "    Args:\n",
    "        predicted_rho (list): List of predicted density matrices\n",
    "        rho_true (list): True density matrix.\n",
    "    \"\"\"\n",
    "    num_examples = len(predicted_rho)\n",
    "\n",
    "    fig, grid = plt.subplots(2, num_examples,\n",
    "                  figsize=((6/4)*num_examples, 2.8), sharex=True, sharey=True)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        plot_wigner(pred_dm[i], ax=grid[0, i])\n",
    "        plot_wigner(rho_true[i], ax=grid[1, i])\n",
    "\n",
    "        grid[0, i].set_title(\"Fidelity {:.2f}\".format(fidelity(rho_true[i], pred_dm[i])))\n",
    "        \n",
    "        if i > 0:\n",
    "            grid[0, i].set_ylabel(\"\")\n",
    "            grid[0, i].set_xlabel(\"\")\n",
    "        grid[0, 0].set_xlabel(\"\")            \n",
    "        if i > 0:\n",
    "            grid[1, i].set_ylabel(\"\")\n",
    "            grid[1, i].set_title(\"\")\n",
    "        grid[1, 0].set_title(\"\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for rhos in generate_batch(batch_size):\n",
    "    rho_tf = dm_to_tf(rhos)\n",
    "    x = batched_expect(ops_batch, rho_tf)\n",
    "    pred_dm = tf_to_dm(model_dm([A, x]))\n",
    "\n",
    "show_comparision(pred_dm[:4], rhos[:4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "We train the networks using TensorFlow optimizers and record the loss function as well as intermediate density matrices during training. In order to get the intermediate density matrices, we construct a `density matrix` model called `model_dm` model that outputs the density matrix using the generator."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class LossHistory:\n",
    "    \"\"\"Class for keeping track of loss\"\"\"\n",
    "    generator: list\n",
    "    discriminator: list\n",
    "    l1: list\n",
    "\n",
    "loss = LossHistory([], [], [])\n",
    "fidelities = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(0.0002,\n",
    "                                                             decay_steps=10000,\n",
    "                                                             decay_rate=0.96,\n",
    "                                                             staircase=False)\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr_schedule, 0.5, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr_schedule, 0.5, 0.5)\n",
    "\n",
    "lam = 100.\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr_schedule, 0.5, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr_schedule, 0.5, 0.5)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reconstruction using QST-CGAN\n",
    "\n",
    "Note that we are comparing fidelity to the ideal state which is clearly going \n",
    "to be rather poor sin"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_step(A, x):\n",
    "    \"\"\"Takes one step of training for the full A matrix representing the\n",
    "    measurement operators and data x.\n",
    "\n",
    "    Note that the `generator`, `discriminator`, `generator_optimizer` and the\n",
    "    `discriminator_optimizer` has to be defined before calling this function.\n",
    "\n",
    "    Args:\n",
    "        A (tf.Tensor): A tensor of shape (m, hilbert_size, hilbert_size, n x 2)\n",
    "                       where m=1 for a single reconstruction, and n represents\n",
    "                       the number of measured operators. We split the complex\n",
    "                       operators as real and imaginary in the last axis. The \n",
    "                       helper function `convert_to_real_ops` can be used to\n",
    "                       generate the matrix A with a set of complex operators\n",
    "                       given by `ops` with shape (1, n, hilbert_size, hilbert_size)\n",
    "                       by calling `A = convert_to_real_ops(ops)`.\n",
    "\n",
    "        x (tf.Tensor): A tensor of shape (m, n) with m=1 for a single\n",
    "                       reconstruction and `n` representing the number of\n",
    "                       measurements. \n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator([A, x], training=True)\n",
    "\n",
    "        disc_real_output = discriminator([A, x, x], training=True)\n",
    "        disc_generated_output = discriminator([A, x, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, x,\n",
    "            lam=lam)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    slopes = tf.convert_to_tensor([tf.sqrt(tf.reduce_sum(s**2)) for s in discriminator_gradients])\n",
    "    gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator([A, x], training=True)\n",
    "\n",
    "        disc_real_output = discriminator([A, x, x], training=True)\n",
    "        disc_generated_output = discriminator([A, x, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, x,\n",
    "            lam=lam)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output) + 100*gradient_penalty\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    loss.generator.append(gen_gan_loss)\n",
    "    loss.l1.append(gen_l1_loss)\n",
    "    loss.discriminator.append(disc_loss)\n",
    "\n",
    "max_iterations = 500\n",
    "pbar = tqdm(range(max_iterations))\n",
    "\n",
    "for i in pbar:\n",
    "    for rhos in generate_batch(batch_size):\n",
    "        rho_tf = dm_to_tf(rhos)\n",
    "        x = batched_expect(ops_batch, rho_tf)\n",
    "        train_step(A, x)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for rhos in generate_batch(batch_size):\n",
    "    rho_tf = dm_to_tf(rhos)\n",
    "    x = batched_expect(ops_batch, rho_tf)\n",
    "    pred_dm = tf_to_dm(model_dm([A, x]))\n",
    "\n",
    "show_comparision(pred_dm[:5], rhos[:5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f_values = []\n",
    "\n",
    "for i in range(len(pred_dm)):\n",
    "    f_values.append(fidelity(pred_dm[i], rhos[i]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.hist(f_values,\n",
    "             bins=10, color=\"darkgreen\",\n",
    "             orientation='horizontal')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Fidelity\", labelpad = -0.1)\n",
    "plt.xlabel(\"# States\", labelpad = -0.1)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2233be6a3956c874a48c6fd5b87bf8a1e6d820c7a8b203922feb36e9920695d6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('qstcgan': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}